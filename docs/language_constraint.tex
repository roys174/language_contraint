%
% File acl2017.tex
%
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2017}
\usepackage{times}
\usepackage{latexsym}
\usepackage{xcolor}
\usepackage{url}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand{\secref}[1]{Section~\ref{ssec:#1}}
\newcommand{\isection}[2]{\section{#1}\label{ssec:#2}}
\newcommand{\isectionb}[1]{\section{#1}\label{ssec:#1}}

\newcommand{\ms}[1]{{\color{cyan}\{\textit{#1}\}$_{ms}$}}


\title{???}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
People's writing style is affected by many factors, including topics, sentiment, and individual personality. 
In this paper we show that writing tasks that impose constraints on the writer result in the author adopting a  different writing style compared to tasks that do not.
As a case study, we experiment with a recently published machine reading task: the story cloze task \cite{Mostafazadeh:2016}. 
In this task, annotators were asked to generate two sentences: one which makes sense given a previous paragraph and another which doesn't.
We show that a linear classifier, which applies only simple style features, such as sentence length and character n-grams, obtains state-of-the-art results on the task,
substantially higher than sophisticated deep learning models.
Importantly, our model doesn't even look at the previous paragraph, just the two candidate sentences, which, out of context, differ only in the constraint put on the authors. 
Our results indicate that such constraints dramatically affect the way people write. 
They also suggest that careful attention to the instructions given to the authors needs to taken when designing new NLP tasks.

\end{abstract}

\section{Introduction}
Writing style is defined as the choice of words, spelling, grammar and punctuation made by the author.\footnote{\url{https://en.wikipedia.org/wiki/Writing_style}}
It is often affected by inter-writer factors such as age \cite{Schler:2006}, gender \cite{Argamon:2003}, native language \cite{Koppel:2005}, or mere personality \cite{Stamatatos:2009}, but also by other parameters such as the sentiment of the text \cite{Davidov:2010} and whether it is sarcastic \cite{Tsur:2010}.  
In this paper we study to what extent is writing style affected by more intricate factors, such as the type of constraints put on the author. 

As a testbed, we experiment with the story cloze task \cite{Mostafazadeh:2016}. 
This task was created by having authors write five-sentence self-contained stories.
Following, a subset of the the stories was given to another group of authors, who were shown only the first four sentences of each story, 
and were asked to write two one-sentence endings for it: a {\it correct} ending, and a {\it wrong} ending.
The goal of the task is to determine which of the endings is the correct one.

Interestingly, although originally intended to be a machine reading task, the compilation of this task raises several research questions which seem to differ from the original intent of the designers.
First, do authors use different style when asked to write a {\it correct} sentence, compared to a {\it wrong} sentence?
Second, do authors use different style when writing the ending as part of their own five sentence story, compared to reading four sentences, and then writing a standalone (correct) ending?

We show that the answer to both of these questions is positive. 
We train a linear classifier, using simple stylistic features, such as sentence length, character n-grams and PoS counts. 
We show that on a balanced dataset (random guess is 50\%) our classifier can distinguish between {\it correct} and {\it wrong} sentences in 64.5\% of the cases. 
Importantly, the classifier is trained {\bf only} on the last sentences, and does not consider the four input sentences.
It is also trained on a set of positive samples and a set of negative ones, rather than pairs of (positive, negative) pairs, as in the original story cloze task.
Furthermore, when trained to distinguish between original endings and new endings, the classifier obtains 70.9\% accuracy. 


In order to estimate the quality of our results, we turn back to the story cloze task.
We show that using our classifier, we are able to obtain 71.6\% accuracy on the task, a 12\% improvement compared to the published state-of-the-art results \cite{Speer:2016}.\footnote{Recently, a shared task for the story cloze task has been published (\url{https://competitions.codalab.org/competitions/15333}). 
At the time of submission, the leading results is 71.1\%, which is much closer to our results, although still inferior.
No details about the methods used to generate this result are available.}

We present an ablation study which shows that the style differences are realized in syntactic features (such as the over/under use of coordination words like ``and'' and ``but'').
Furthermore, we also show that sentiment plays an important role in the writing style differences.
For instance, one of the key features for distinguishing between correct and wrong sentences is the over-representation of the word ``hate" by the latter.

Our results suggest that writing style is affected by the the writer's state of mind.
Writing a sentence intended to be {\it wrong} turns out quite differently than a sentence intended to be {\it correct}. 
Similarly, writing a sentence as part of the story is different from reading a story, and then writing the final sentence.
These differences can be distinguished to a large extent by simple machine learning tools.

The results presented here also provide valuable lessons for designing new NLP tasks.
Although \cite{Mostafazadeh:2016} seem to have put a lot of effort into designing the task, 
addressing many potential methodological flaws (see \secref{ROC_Story}), the importance of a few allegedly minor details were underestimated. 
%We show that these details are actually very significant.

Finally, we show that our stylistic features can benefit from combining with a machine-reading model, for which this task was designed.
We train an neural language model on the original five sentence training corpus, and then compute the language probability of each of the candidates answers. 
We add the numbers as features in our linear classifier, and get an additional 4\% improvement (75.6\%).

the reminder of this paper is organized as follows. In \secref{ROC_Story} we introduce the cloze story task.
We present our model, our experiments and our results at sections \ref{ssec:Model}, \ref{ssec:Experiments} and \ref{ssec:Results}, respectively.
Sections \ref{ssec:Ablation} and \ref{ssec:Discussion} present an ablation study and a discussion, while \secref{Related}  surveys related work. 
We conclude at \secref{Conclusion}.

\isection{The Cloze Story Task}{ROC_Story}
Developed as an effort to simplify the representation and learning of commonsense knowledge, the \textit{Cloze Story Task} \cite{Mostafazadeh:2016} has become the Shared Task for the LSDSem workshop. % this is a terrible sentence.
The task provides two types of datasets: the \textit{ROC Stories} and the \textit{Story Cloze test sets}.

\paragraph{ROC Stories}
consist of $98,163$ five-sentence commonsense stories, collected on AMT. Workers were instructed to write a coherent story where something happens, and with a clear beginning and end. To collect a broad spectrum of commonsense knowledge, there was no imposed subject for the stories.
\paragraph{Story Cloze} test sets were created on AMT, using a subset of ROC Stories. Presented with the first four sentences of a story, workers were asked to write a ``right'' and a ``wrong'' ending. The ending had to complete the story using a character in it, and when read out of context, had to be ``realistic and sensible'' \cite{Mostafazadeh:2016}.

The resulting stories were then individually rated for coherence and meaningfulness by AMT workers. Only stories with a coherent and meaningful ``right'' ending and a neutral ``wrong'' ending were selected for the test, yielding $3,744$ test stories. \ms{Hope the ``neutral'' thing makes sense, I didn't know how else to explain 4.2.2 of the Story cloze paper.}

\isectionb{Model}

\subsection{Neural Language Model}
\ms{Below is a messy dump of all the hyper parameters... Didn't know how many details were needed}

We trained a simple RNN Language model \cite{mikolov2010recurrent} using a single-layer LSTM \cite{hochreiter1997long} of hidden dimension $h=512$.
We used the ROC Stories for training, setting aside $10\%$ for validation of the language model. We replaced all words occuring less than 3 times by a special out-of-vocabulary character, yielding a vocabulary size of $|V|=21,582$.
Only during training, we applied a dropout rate of 60\% while running the LSTM over all 5 sentences of the stories. Using AdamOptimizer \cite{kingma2014adam} and a learning rate of $\eta=.001$, we trained with backpropagation on cross-entropy. % minibatches of $50$ stories.
% h=512, dropout rate of 60%, V=21582, 10% validation set

To construct features for the Story Cloze task, we scored each of the two endings using our neural language model. We computed the probability of each ending given the first four sentences $p_\theta(ending|story)$, as well as the probability of the endings out of context $p_\theta(ending)$. We also included the likelihood ratio $\frac{p_\theta(ending|story)}{p_\theta(ending)}$ into our classifier.
\isectionb{Experiments}

\isectionb{Results}

\isection{Ablation Study}{Ablation}

\isectionb{Discussion}

\isection{Related Work}{Related}

\begin{itemize}
\item Different style application (as in introduction). Also include deception works (Yejin has 1-2 papers on it). 
\item Machine reading papers? 
\item \ms{Pennebaker's work on function words?}
\end{itemize}


\isectionb{Conclusion}




% include your own bib file like this:
%\bibliographystyle{acl}
%\bibliography{acl2017}

\newpage
\bibliography{acl2017}
\bibliographystyle{acl_natbib}

\end{document}
