\documentclass[a4paper,11pt]{article}
\usepackage{authblk}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym,xcolor}
\usepackage{multirow}


%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}
\renewcommand{\Authands}{ and }
% Editing macros
\newcommand{\my}[1]{\footnote{\color{red}{\textbf{#1}}}}

\newcommand{\ms}[1]{{\color{cyan}\{\textit{#1}\}$_{ms}$}}
\newcommand{\roy}[1]{{\color{orange}\textsc{[#1 --rs]}}}
\newcommand{\royb}[2]{{\color{red}{\sout{#1}}}{\color{green}{#2}}}
\newcommand{\royc}[3]{\royb{#1}{#2}\roy{#3}}
\newcommand{\yc}[1]{{\color{bblue}\{\textit{#1}\}$_{yc}$}}
\newcommand{\nascomment}[1]{\textcolor{blue}{[\textsc{#1 --nas}]}}
\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\com}[1]{}
\newcommand{\resolved}[1]{}
\definecolor{forestgreen}{rgb}{0.13, 0.55, 0.13}
\newcommand{\isubsectionb}[1]{\subsection{#1}\label{ssec:#1}}
\newcommand{\secref}[1]{Section~\ref{ssec:#1}}

\usepackage{eacl2017}
\eaclfinalcopy % Uncomment this line for the final submission
%\def\eaclpaperid{***} %  Enter the acl Paper ID here


\title{Story Cloze Task: UW NLP System}

\author[1,2]{\bf Roy Schwartz}
\author[1]{\bf Maarten Sap}
\author[1]{\bf Ioannis Konstas}
\author[1]{\\ \bf Leila Zilles}
\author[1]{\bf Yejin Choi}
\author[1]{\bf Noah A.~Smith}
\affil[1]{Computer Science \& Engineering, University of Washington, Seattle, WA 98195, USA}
\affil[2]{Allen Institute for Artificial Intelligence, Seattle, WA 98103, USA}
\affil[  ]{\tt \{roysch,msap,ikonstas,lzilles,yejin,nasmith\}@cs.washington.edu}
\date{}

\begin{document}
\maketitle
\begin{abstract} 
This paper describes University of Washington NLP's submission for the Linking Models of Lexical, Sentential and Discourse-level Semantics (LSDSem 2017) shared task---the \textit{Story Cloze Task}.
Our system is a linear classifier with a variety of features, including both the scores of a neural language model and style features.
We report  $75.2\%$ accuracy on the task. 
A further discussion of our results can be found in \newcite{Schwartz:2017}.
\end{abstract}

\section{Introduction}
As an effort to advance commonsense  understanding, \newcite{Mostafazadeh:2016} developed the \textit{story cloze task}, which is the focus of the LSDSem 2017 shared task. 
In this task, systems are given two short, self-contained stories, which differ only in their last sentence: 
one has a {\it right} (coherent) ending, and the other has a {\it wrong} (incoherent) ending.
The task is to tell which is the {\it right} story.
In addition to the task, the authors also introduced the {\it ROC story corpus}---a training corpus of five-sentence (coherent) stories. 
\tabref{ROC-example} shows an example of a {\it coherent} story and an {\it incoherent} story from the story cloze task.


In this paper, we describe University of Washington NLP's submission for the shared task.
Our system explores several types of features for the task.
First, we train a neural language model \cite{mikolov2010recurrent} on the ROC story corpus. 
We use the probabilities assigned by the model to each of the endings ({\it right} and {\it wrong}) as classification features.

Second, we attempt to distinguish between {\it right} and {\it wrong} endings using style features, such as sentence length, character $n$-grams and word $n$-grams. 
Our intuition is that the {\it right} endings use a different style compared to the {\it wrong} endings.
The features we use were shown useful for style detection in tasks such as age \cite{Schler:2006}, gender \cite{Argamon:2003}, 
and authorship profiling \cite{Stamatatos:2009}.

We feed our features to  a logistic regression classifier, and evaluate our system on the shared task.
Our system obtains $75.2\%$ accuracy on the test set.
Our findings hint that the different writing tasks used to create the story cloze task---writing {\it right} and {\it wrong} endings---impose different writing styles on authors. 
This is further discussed in \newcite{Schwartz:2017}.

\begin{table}[!t]
%\small
\begin{tabular}{|p{3.5cm}|p{3.25cm}|} \hline
{\bf Story Prefix} & {\bf Ending} \\ \hline
\multirow{2}{*}{\parbox[b][7.5em][c]{3.5cm}{Kathy went shopping. She found a pair of great shoes. The shoes were \$300. She bought the shoes.}}	&{\color{blue}{She felt buyer's remorse after the purchase.}} \\ \cline{2-2}
& {\color{red}{Kathy hated buying shoes.}} \\ \hline
\end{tabular}
\caption{\label{ROC-example}
Examples of stories from the story cloze task \cite{Mostafazadeh:2016}. 
The left column shows that first four sentences of a story. 
The right column shows two contrastive endings for the story:
  a {\color{blue}{coherent}} ending (upper row) and a {\color{red}{incoherent}} one (bottom row).
}
%\end{center}
\end{table}


\section{System Description}
We design a system that predicts, given a pair of story endings, which is the  \textit{right} one and which is the \textit{wrong} one.
Our system applies a linear classifier guided by several types of features to solve the task.
We describe the system in detail below.

\subsection{Model}
We train a binary logistic regression classifier to distinguish between {\it right} and {\it wrong} stories. 
We use the set of {\it right} stories as positive samples and the set of {\it wrong} stories as negative samples.
At test time, for a given pair, we consider the classification results of both candidates. 
If our classifier assigns different labels to each candidate, we keep them.  
If not, the label whose posterior probability is lower is reversed.
We describe the classification features below.

\isubsectionb{Features}
We use two types of features, designed to capture different aspects of the problem. 
We use \textit{neural language model} features to leverage corpus level word distributions, specifically longer term sequence probabilities.
We use \textit{stylistic} features to capture differences in writing between {\it coherent} story endings and {\it incoherent} ones. 


\paragraph{Language model features.}

We experiment with state-of-the-art text comprehension models,
specifically an LSTM \cite{hochreiter1997long} recurrent neural
network language model (RNNLM; \nocite{mikolov2010recurrent}Mikolov et
al., 2010).
Our RNNLM is used to generate two different probabilities:
$p_\theta(\textrm{ending})$, which is the language model probability of the fifth sentence alone and $p_\theta(\textrm{ending} \mid \textrm{story})$, which is the RNNLM probability of the fifth sentence given the first four sentences.  
We use both of these probabilities as classification features.

In addition, we also apply a third feature:
\begin{equation}
\frac{p_\theta(\textrm{ending} \mid
  \textrm{story})}{p_\theta(\textrm{ending})} \label{eq:ratio}
\end{equation}

The intuition is that a \emph{correct} ending should be unsurprising (to the model) given the four preceding sentences of the story (the numerator), controlling for the inherent surprise of the words in that ending (the denominator).\footnote{Note that taking the logarithm of
the expression in Equation~\ref{eq:ratio} gives the pointwise mutual information between the story and the ending, under the language model.}


\paragraph{Stylistic features.}
We hypothesize that {\it right} and {\it wrong} endings might be distinguishable using style features.
We adopt style features that have been shown useful in the past in tasks such as detection of age \cite{Schler:2006,Rosenthal:2011,nguyen:2011:latech}, gender  \cite{Argamon:2003,Schler:2006,bamman2014gender}, and native language
\cite{Koppel:2005,Tsur:2007,Bergsma:2012}.

We add the following classification features to capture style differences between the two endings. 
These features are computed on the story endings alone ({\it right} or {\it wrong}), and do not consider, either at train or at test time, the first four (shared) sentences of each story.
\begin{itemize}
\item\textit{\textbf{Length}.} The number of words in the sentence.
\item\textit{\textbf{Word $n$-grams.}} We use sequences of 1--5
  words. Following \newcite{Tsur:2010} and \newcite{Schwartz:2013}, we distinguish between high frequency and low frequency words. 
Specifically, we replace content words, which are often low frequency, with their part-of-speech tags (Nouns, Verbs, Adjectives, and Adverbs).
\item\textit{\textbf{Character $n$-grams.}} Character $n$-grams are useful features in the detection of author style \cite{Stamatatos:2009} or language identification \cite{lui2011cross}.
We use character 4-grams.
\end{itemize}



\subsection{Experimental Setup}
The story cloze task doesn't have a training corpus for the {\it right} and {\it wrong} endings.
Therefore, we use the development set as our training set, holding out 10\% for development
(3,366 training endings, 374 for development). We keep the story cloze test set as is (3,742 endings).

We use  Python's sklearn logistic regression implementation with $L_2$
regularization, performing grid search on the development set to
tune a single hyperparameter---the regularization parameter.  

For computing the RNN features, we start by tokenizing the text using the nltk tokenizer.\footnote{\url{www.nltk.org/api/nltk.tokenize.html}} 
We then use TensorFlow\footnote{\url{www.tensorflow.org}} to train the RNNLM using a single-layer LSTM of hidden dimension 512.
We use the ROC Stories for training, setting aside 10\% for validation of the language model.\footnote{We train on both the Spring 2016 and the Winter 2017 datasets, a total of roughly 100K stories.}
We replace all words occurring less than 3 times by a special out-of-vocabulary character, yielding a vocabulary size of 21,582.
Only during training, we apply a dropout rate of 60\% while running the LSTM over all 5 sentences of the stories. 
Using Adam optimizer \cite{kingma2014adam} and a learning rate of
$\eta=.001$, we train to minimize cross-entropy.
The resulting RNN features (see \secref{Features}) are taken in log space.

For  the style features, we add a \textsc{start} symbol at the beginning
of each sentence.\footnote{Virtually all sentences end with a period
  or an exclamation mark, so we do not add a \textsc{stop} symbol.} 
 We keep $n$-gram (character or word) features that occur at least five times in the training set.
All stylistic feature values are normalized to the range [0, 1].
For the part-of-speech features, we tag all endings with the Spacy POS tagger.\footnote{\url{spacy.io/}}
The total number of features used by our system is 7,651.


\section{Results}

\begin{table}%[!t]
\begin{center}
%\small
\begin{tabular}{|l|r|} \hline
{\bf Model} & {\bf Acc.} \\ \hline
{DSSM} \cite{Mostafazadeh:2016} & 0.585 \\ 
{LexVec} \cite{Salle:2016} & 0.599 \\ \hline\hline
{RNNLM features}		& 0.677 \\ 
{Stylistic features} & {0.724} \\ 
{\bf Combined (Style + RNNLM)} & {\bf 0.752} \\ \hline\hline
Human judgment & 1.000 \\ \hline
\end{tabular}
\end{center}
\caption{\label{cloze_results}
Results on the test set of the  story cloze task. 
The first block are published results, the second block are our results.
LexVec results are taken from \cite{Speer:2016}.
Human judgement scores are taken from \cite{Mostafazadeh:2016}. 
}
\end{table}

% Show results
The performance of our system is described in Table \ref{cloze_results}. 
With $75.2\%$ accuracy, our system achieves $15.3\%$ better than the published state of the art \cite{Salle:2016}. 
The table also shows an analysis of the different features types used by our system.
While our RNNLM features alone reach 67.7\%, the style features perform better---72.4\%. 
This suggests that while this task is about story understanding, 
there is some information contained in stylistic features, which are slightly less sensitive to content.
As expected, the RNNLM features complement the stylistic ones, boosting performance by 7.5\% (over the RNNLM features) and 2.8\% (over the style features). 

In an attempt to provide explanation to the strong performance of the stylistic feature, 
we hypothesize that the different writing tasks---writing a {\it right} and a {\it wrong} ending---impose a different style on the authors, which is expressed in the different style adopted in each of the cases. 
The reader is referred to \newcite{Schwartz:2017} for more details and discussion.


\section{Conclusion}
This paper described University of Washington NLP's submission to the LSDSem 2017 Shared Task. 
Our system leveraged both neural language model features and  stylistic features, achieving $75.2\%$ accuracy on the classification task. 

\section*{Acknowledgments}
The authors thank the shared task organizers and anonymous reviewers
for feedback. This research was supported in
part by DARPA under the Communicating with Computers program.

\bibliography{eacl2017}
\bibliographystyle{eacl2017}


\end{document}