\documentclass[11pt]{article}
\usepackage{eacl2017}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym,xcolor}

% \eaclfinalcopy % Uncomment this line for the final submission
%\def\eaclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}
% Editing macros
\newcommand{\my}[1]{\footnote{\color{red}{\textbf{#1}}}}

\newcommand{\ms}[1]{{\color{cyan}\{\textit{#1}\}$_{ms}$}}
\newcommand{\roy}[1]{\footnote{\color{red}{\textbf{Roy: #1}}}}
\newcommand{\royb}[2]{{\color{red}{\sout{#1}}}{\color{green}{#2}}}
\newcommand{\royc}[3]{\royb{#1}{#2}\roy{#3}}
\newcommand{\yc}[1]{{\color{bblue}\{\textit{#1}\}$_{yc}$}}

\title{Story Cloze task -- UW System}

\author{Roy Schwartz\\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And	
  Maarten Sap \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract} % This is my real quick try at this lol
As an effort to further commonsense short story understanding, Mostafazadeh, et al. ~(2016)\nocite{Mostafazadeh:2016} developed the \textit{Story Cloze Task}, now the 2017 LSD Sem shared task. This paper describes the University of Washington's submitted system.
We use stylistic and text comprehension features to determine which of two endings best completes a short story.
We achieve $75.2\%$ accuracy.
\end{abstract}

\section{Introduction}
\ms{note: I don't know if we're emphasizing ``story understanding'' too much, given that we're really not doing that?}

Learning of commonsense knowledge is one of AI's biggest challenges, since it is not usually found in knowledge bases.
Coming up with the right types of data to analyze has been hard, and models are also not working well.

One particular type of commonsense knowledge that has gotten attention is story understanding \cite{??}. 
Stories have narratives that require understanding of how events typically flow and of what events are coherent after others.
Related to understanding event sequences is script learning, which focuses on stereotypical chains of events.

% Mention the specific purpose of the shared task (i.e. from the website)
The 2017 LSD Sem Shared Task provide a testbed to further commonsense story understanding: the \textit{Story Cloze} task \cite{Mostafazadeh:2016}. Specifically designed for the purpose of facilitating the learning of commonsense knowledge, the task consist of finding the \textit{corrent} ending to four sentence short stories, out of two possible endings.
% Along with the two-ending stories, Mostafazadeh, et al. ~(2016)\nocite{Mostafazadeh:2016} also released nearly $100$k five-sentence short stories (ROC Stories), to facilitate learning of commonsense narratives.

% Last paragraph should be a quick description of our system.
In this paper, we describe the system submission from the University of Washington NLP group (UW NLP).
We implement a simple classifier that uses surface features as well as language model features. Using the Shared Task test set, we achieve $75.2\%$ accuracy.

\section{Task summary}
% this section helps clarify some of the nomenclature?

The \textit{Story Cloze Task} \cite{Mostafazadeh:2016}, selected as the 2017 LSD Sem Shared Task, aims to test commonsense story understanding. Towards that goal, two waves of short story collection were done.

\ms{Below is mostly copy-pasted from the paper, with a little of editing to remove our langConstraint angle.}
\paragraph{ROC Stories.}
The ROC Story Corpus consists of 49,255 five-sentence commonsense stories, collected on Amazon Mechanical Turk (AMT).\footnote{Recently, an additional 53K stories were released, which results in roughly 100K stories.}
Workers were instructed to write a coherent self-contained story, which has a clear beginning and end. 
To collect a broad spectrum of commonsense knowledge, there was no imposed subject for the stories,
which resulted in a wide range of different topics.
\paragraph{Story Cloze Task.}
After compiling the story corpus, the {\it Story Cloze Task} -- a task based on the corpus -- was introduced.
A subset of the stories was selected, and only the first four sentences of each story were presented to AMT workers.
Workers were asked to write a pair of new story endings for each story context: a {\it right} one and a {\it wrong} one.
Both endings are required to complete the story using one of the characters in the story context. 
Additionally, the ending is required to be ``realistic and sensible'' \cite{Mostafazadeh:2016} when read out of context.

The resulting stories, both {\it right} and {\it wrong}, were then individually rated for coherence and meaningfulness. 
Only stories rated as simultaneously coherent with a {\it right} ending and neutral with a {\it wrong} ending were selected for the task. 

Based on these new stories, the task becomes: given a pair of stories that differ only in their endings, how well can the system determine which ending is {\it right} and which is {\it wrong}?



\subsection{Data}
For the Shared task, Mostafazadeh, et al. ~(2016)\nocite{Mostafazadeh:2016} split the Story Cloze stories into a development and a test set, each containing $1,871$ stories.
While the ROC Stories provide data to do unsupervised learning of how stories should end, our goal is to be able to determine whether an ending is the \textit{right} one or the \textit{wrong} one. To that extent, we use the development set to train a classifier using features of the individual endings. We split that development set (90/10) into training and development.
The final size of our training/development/test sizes are 3,366/374/3,742 endings, respectively. 

It is worth nothing that our classification task is a slightly different take on the {\it Story cloze task}. 
Instead of classifying pairs of endings, one which is {\it right} and another which is {\it wrong}, we take the set of  {\it right} endings as positive samples and the set of {\it wrong} endings as our negative examples. 


\section{System Description}
%% Explain that we have two sets of features, style and language.
%% Maybe mention here that we're doing training on validation set ?
In our system, we implement a simple linear classifier to determine whether and ending is the \textit{correct} one or the \textit{incorrect} one.
Specifically, we use the logistic regression implementation by Scikit-Learn \cite{scikit-learn}, regularizing using an L2 loss.
We grid search the regularization parameter on our development set. 
\ms{We should maybe add one of two more sentences?}

\subsection{Features}
We use two types of features (described below), designed to capture different dimensions of the problem. We use \textit{stylistic} features to capture differences in writing between ``coherent'' endings and ``incoherent'' ones. We opt for \textit{language model} features to leverage corpus level word distributions, specifically longer term sequence probabilities.
Our end system makes use of $XX$ distinct features. \ms{Would be nice to add that number?}
\paragraph{Stylistic Features}
%% take stuff from the paper
In our classification task, we add the following features to capture style differences between the two endings:
\begin{itemize}
\item\textit{\textbf{Length}.} The number of words in the sentence.
\item\textit{\textbf{Word n-grams.}} We use sequences of 1-5 words. Following \cite{Tsur:2010,Schwartz:2013}, we distinguish between high frequency and low frequency words. 
Specifically, we replace content words, which are often low frequency, with their part-of-speech tags (Nouns, Verbs, Adjectives and Adverbs).
\item\textit{\textbf{Character n-grams.}} Character n-grams are useful features in author style \cite{Stamatatos:2009} or language identification \cite{lui2011cross}.
We use character 4-grams.
\end{itemize}
For computing our features, we keep n-gram (character or word) features that occur at least five times in the training set.
All stylistic feature values are normalized to [0-1].
For the POS features, we tag all endings with the Spacy POS tagger.\footnote{\url{spacy.io/}}

\paragraph{Language Model Features}
%% take the RLM paragraph from the paper
In addition to stylistic features, we experiment with state-of-the-art text comprehension models, specifically a recurrent neural network language model (RNNLM; Mikolov, et al. (2010)\nocite{mikolov2010recurrent}).
We train the RNNLM using a single-layer LSTM \cite{hochreiter1997long} of hidden dimension 512.
We use the ROC Stories for training, setting aside 10\% for validation of the language model. 
We replace all words occurring less than 3 times by a special out-of-vocabulary character, yielding a vocabulary size of 21,582.
Only during training, we apply a dropout rate of 60\% while running the LSTM over all 5 sentences of the stories. 
Using AdamOptimizer \cite{kingma2014adam} and a learning rate of
$\eta=.001$, we train to minimize cross-entropy.

In our classifier, we include three different scores given by the language model:
$p_\theta(\textrm{ending})$, $
p_\theta(\textrm{ending} \mid \textrm{story})$, $
\frac{p_\theta(\textrm{ending} \mid \textrm{story})}{p_\theta(\textrm{ending})}$.

The intuition is that a \emph{right} ending should be unsurprising (to the model)given the four preceding sentences of the story (the numerator), controlling for the inherent surprisingness of the words in that ending (the denominator). \ms{@roy, did we try a classifier trained on all three features only?}

\section{Results \& Discussion}

\begin{table}%[!t]
\begin{center}
%\small
\begin{tabular}{|l|r|} \hline
{\bf Model} & {\bf Accuracy} \\ \hline
{DSSM} \cite{Mostafazadeh:2016} & 0.585 \\ 
{LexVec} \cite{Salle:2016} & 0.599 \\ \hline\hline
%{Niko (shared task)}	& 0.700\\ 
%{tbmihaylov (shared task)} & 0.711\\ \hline\hline
{RNNLM}		& 0.677 \\ 
{Stylistic features} & {0.724} \\ 
{\bf Combined (Style + RNNLM)} & {\bf 0.752} \\ \hline\hline
Human judgment & 1.000 \\ \hline
\end{tabular}
\end{center}
\caption{\label{cloze_results}
Results on the test set of the  story cloze task. 
The first block are published results, the second block
as-yet-unpublished results from the LSDSem 2017 leaderboard, the third
block ours.
LexVec results are taken from \cite{Speer:2016}.
RNN is our implementation. 
Human judgement scores are taken from \cite{Mostafazadeh:2016}. 
}
\end{table}

% Show results
The performance of our system is described in Table \ref{cloze_results}. With $75.2\%$ accuracy, our system achieved $15.3\%$ better than the published state of the art by Salle et al. (2016) \nocite{Salle:2016}. 
\ms{At the time of submission, we beat everyone or we're ranked XX in the leaderboard.}
Ablation analysis shows that stylistic features have the most predictive power.
This suggests that while this task is about story understanding, there is some information contained in stylistic features, which are slightly less sensitive to content.
As expected, RNNLM features complement the stylistic ones, boosting performance by $2.8\%$. This suggests that while the stylistic feaures provide useful signal, content-oriented models also contribute information. 
\ms{Perhaps, an even more complex story understanding model would provide an even greater boost.}


\subsection{Further analysis} 

\begin{table}[h]
\begin{center}
%\small
\begin{tabular}{|c|c|} \hline
\textit{\textbf{Right}} & \textit{\textbf{Wrong}}\\ \hline
`ally' & ` hate'\\ \hline
`VBD the' & ` hat'\\ \hline
`START RB' & `START NNP'\\ \hline
`ved ' & `ated'\\ \hline
` tim' & `NN .'\\ \hline

\end{tabular}
\end{center}
\caption{\label{exp1_features}}
The top 5 most discriminative features for predicting {\it right} vs.~{\it wrong} endings.\end{table}

\ms{Don't know if this steps onto our other paper's toes.}
We analyze which features are most predictive of a \textit{wrong} or \textit{right} ending.
Table \ref{exp1_features} shows the highest weighted features using the logistic regression coefficients. 
The table shows a few interesting trends. 
First, authors tend to structure their sentences differently when writing {coherent}  vs. {incoherent} endings.
For instance, {coherent} endings are more likely to start with an adverb (e.g., ``then'', ``so'', ``eventually''), while {incoherent} ones tend to start with a proper noun.
In addition, we find that {incoherent} endings are more likely to
finish the sentence with a common noun.  %\nascomment{note to self to come backto this after we agree on methodology.}

More interestingly, the different writing tasks seem to impose a specific sentiment on the writer. 
Three of the top four most salient features for detecting {\it wrong} endings are variants of the verb ``hate''.
This indicates that when authors are asked to write {\it wrong} text, they tend to use negative language.

An alternative explanation to this hypothesis might be that the first four sentences of the stories in the ROC story corpus tend to be positive, and thus in order to make an ending {\it wrong}, authors adopted a negative approach. 
A similar idea was suggested in the original story cloze paper, where two sentiment-based baselines were evaluated. 
These baselines measured the relative sentiment between the ending and the previous sentences.
The performance of both these baselines was roughly chance-level, which seems to suggest that this is not entirely the case.


% ablation study? Table 2 from the paper sort of?

\section{Conclusion}
This paper described the UW NLP's submission to the 2017 LSD Sem Shared Task. 
% Our system makes use of knowledge brought to us by the constrained writing literature (?)
% talk about how we use state-of-the-art language modelling tools.
Our system leveraged stylistic features and text comprehension features, achieving $75.2\%$ accuracy on the classification task. We used standard NLP features to capture the style of short story endings, and state of the art language model tools for content-sensitive features.
% We achieve $75.2\%$ which is ranked XXth in the ongoing codalab competition?
% Say something like:
While we achieved \ms{(near?)} state of the art performance, we still only achieved 75\% accuracy. This means the task is far from being solved. We need better tools to understand the common sense knowledge embedded in these stories.

\bibliography{acl2017}
\bibliographystyle{eacl2017}


\end{document}