\documentclass[11pt]{article}
\usepackage{eacl2017}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym,xcolor}
\usepackage{multirow}

% \eaclfinalcopy % Uncomment this line for the final submission
%\def\eaclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}
% Editing macros
\newcommand{\my}[1]{\footnote{\color{red}{\textbf{#1}}}}

\newcommand{\ms}[1]{{\color{cyan}\{\textit{#1}\}$_{ms}$}}
\newcommand{\roy}[1]{{\color{orange}\textsc{[#1 --rs]}}}
\newcommand{\royb}[2]{{\color{red}{\sout{#1}}}{\color{green}{#2}}}
\newcommand{\royc}[3]{\royb{#1}{#2}\roy{#3}}
\newcommand{\yc}[1]{{\color{bblue}\{\textit{#1}\}$_{yc}$}}

\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\com}[1]{}
\definecolor{forestgreen}{rgb}{0.13, 0.55, 0.13}


\title{Story Cloze task -- XXX System}

\author{Roy Schwartz\\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And	
  Maarten Sap \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract} % This is my real quick try at this lol
This paper describes our system for the Linking Models of Lexical, Sentential and Discourse-level Semantics (LSDSem 2017) shared task -- the \textit{Story Cloze Task}.
Our system feeds a linear classifier with a variety of features, including both the output of a neural language model and style features.
We report  $75.2\%$ accuracy on the task.
\end{abstract}

\section{Introduction}
%\ms{note: I don't know if we're emphasizing ``story understanding'' too much, given that we're really not doing that?}
As an effort to advance commonsense  understanding, \newcite{Mostafazadeh:2016} developed the \textit{story cloze task}, which is the focus of the LSDSem 2017 shared task. 
In this task, systems are given two short, self-contained stories, which differ only in their last sentence: 
one has a {\it right} (coherent) ending, and the other has a {\it wrong} (incoherent) ending.
The task is to tell which is the {\it right} story.
In addition to the task, the authors also introduced the {\it ROC story corpus} --  a training corpus of five-sentence (coherent) stories. 
%The goal of the task is to determine which of the endings is the correct one.
%The goal of this dataset is to serve as a commonsense challenge for NLP and AI research. 
\tabref{ROC-example} shows an example of a {\it coherent} story and an {\it incoherent} story from the story cloze task.


\com{
Learning of commonsense knowledge is one of AI's biggest challenges, since it is not usually found in knowledge bases.
Coming up with the right types of data to analyze has been hard, and models are also not working well.

One particular type of commonsense knowledge that has gotten attention is story understanding \cite{??}. 
Stories have narratives that require understanding of how events typically flow and of what events are coherent after others.
Related to understanding event sequences is script learning, which focuses on stereotypical chains of events.

% Mention the specific purpose of the shared task (i.e. from the website)
The 2017 LSD Sem Shared Task provide a testbed to further commonsense story understanding: the \textit{Story Cloze} task \cite{Mostafazadeh:2016}. Specifically designed for the purpose of facilitating the learning of commonsense knowledge, the task consist of finding the \textit{corrent} ending to four sentence short stories, out of two possible endings.
% Along with the two-ending stories, Mostafazadeh, et al. ~(2016)\nocite{Mostafazadeh:2016} also released nearly $100$k five-sentence short stories (ROC Stories), to facilitate learning of commonsense narratives.
}

% Last paragraph should be a quick description of our system.
In this paper, we describe the system we submitted for the shared task.
Our system explores several types of features for the task.
First, we train a neural language model \cite{mikolov2010recurrent} on the ROC story corpus. 
We use the probabilities assigned by the model to each of the endings ({\it right} and {\it wrong}) as classification features.

Second, we attempt to distinguish between {\it right} and {\it wrong} endings using style features, such as sentence length, character n-grams and word n-grams. 
Our intuition is that the {\it right} endings use a different style compared to the {\it wrong} endings.
The features we use are motivated by previous works on tasks such as age \cite{Schler:2006}, gender \cite{Argamon:2003}, 
and authorship profiling \cite{Stamatatos:2009}, for which similar style features are known to be very effective.

We feed our features to  a logistic regression classifier, and evaluate our system on the shared task.
Our system obtains $75.2\%$ accuracy on the test set.


\begin{table}[!t]
%\small
\begin{tabular}{|p{3.5cm}|p{3.5cm}|} \hline
{\bf Story Prefix} & {\bf Ending} \\ \hline
\multirow{2}{*}{\parbox[b][7.5em][c]{3.5cm}{Kathy went shopping. She found a pair of great shoes. The shoes were \$300. She bought the shoes.}}	&{\color{forestgreen}{She felt buyer's remorse after the purchase.}} \\ \cline{2-2}
& {\color{red}{Kathy hated buying shoes.}} \\ \hline
\end{tabular}
\caption{\label{ROC-example}
Examples of stories from the story cloze task \cite{Mostafazadeh:2016}. 
The left column shows that first four sentences of a story. 
The right column shows two contrastive endings for the story:
 %of the same story: 
 a {\color{forestgreen}{coherent}} ending and a {\color{red}{incoherent}} one.
}
%\end{center}
\end{table}
\com{
\section{Task summary}
% this section helps clarify some of the nomenclature?

The \textit{Story Cloze Task} \cite{Mostafazadeh:2016}, selected as the 2017 LSD Sem Shared Task, aims to test commonsense story understanding. Towards that goal, two waves of short story collection were done.

%\ms{Below is mostly copy-pasted from the paper, with a little of editing to remove our langConstraint angle.}
\paragraph{ROC Stories.}
The ROC Story Corpus consists of 49,255 five-sentence commonsense stories, collected on Amazon Mechanical Turk (AMT).\footnote{Recently, an additional 53K stories were released, which results in roughly 100K stories.}
Workers were instructed to write a coherent self-contained story, which has a clear beginning and end. 
To collect a broad spectrum of commonsense knowledge, there was no imposed subject for the stories,
which resulted in a wide range of different topics.
\paragraph{Story Cloze Task.}
After compiling the story corpus, the {\it Story Cloze Task} -- a task based on the corpus -- was introduced.
A subset of the stories was selected, and only the first four sentences of each story were presented to AMT workers.
Workers were asked to write a pair of new story endings for each story context: a {\it right} one and a {\it wrong} one.
Both endings are required to complete the story using one of the characters in the story context. 
Additionally, the ending is required to be ``realistic and sensible'' \cite{Mostafazadeh:2016} when read out of context.

The resulting stories, both {\it right} and {\it wrong}, were then individually rated for coherence and meaningfulness. 
Only stories rated as simultaneously coherent with a {\it right} ending and neutral with a {\it wrong} ending were selected for the task. 

Based on these new stories, the task becomes: given a pair of stories that differ only in their endings, how well can the system determine which ending is {\it right} and which is {\it wrong}?



\subsection{Data}
For the Shared task, Mostafazadeh, et al. ~(2016)\nocite{Mostafazadeh:2016} split the Story Cloze stories into a development and a test set, each containing $1,871$ stories.
While the ROC Stories provide data to do unsupervised learning of how stories should end, our goal is to be able to determine whether an ending is the \textit{right} one or the \textit{wrong} one. To that extent, we use the development set to train a classifier using features of the individual endings. We split that development set (90/10) into training and development.
The final size of our training/development/test sizes are 3,366/374/3,742 endings, respectively. 

It is worth nothing that our classification task is a slightly different take on the {\it Story cloze task}. 
Instead of classifying pairs of endings, one which is {\it right} and another which is {\it wrong}, we take the set of  {\it right} endings as positive samples and the set of {\it wrong} endings as our negative examples. 
}

\section{System Description}
%% Explain that we have two sets of features, style and language.
%% Maybe mention here that we're doing training on validation set ?
We design a system that predicts, given a pair of story endings, which is the  \textit{right} one and which is the \textit{wrong} one.
Our system applies a linear classifier guided by several types of features to solve the task.
We describe the system in detail below.

\subsection{Model}
We train a binary logistic regression classifier to distinguish between {\it right} and {\it wrong} stories. 
We use the set of {\it right} stories as positive samples and the set of {\it wrong} stories as negative samples.
At test time, for a given pair, we consider the classification results of both candidates. 
If our classifier assigns different labels to each candidate, we keep them.  
If not, the label whose posterior probability is lower is reversed.
We describe the classification features below.

\subsection{Features}
We use two types of features, designed to capture different aspects of the problem. 
We use \textit{neural language model} features to leverage corpus level word distributions, specifically longer term sequence probabilities.
We use \textit{stylistic} features to capture differences in writing between {\it coherent} story endings and {\it incoherent} ones. 


\paragraph{Language Model Features.}
%% take the RLM paragraph from the paper
We experiment with state-of-the-art text comprehension models, specifically an LSTM \cite{hochreiter1997long} recurrent neural network language model (RNNLM; \newcite{mikolov2010recurrent}).
Our RNNLM is used to generate two different probabilities:
$p_\theta(\textrm{ending})$, which is the language model probability of the fifth sentence alone and $p_\theta(\textrm{ending} \mid \textrm{story})$, which is the RNNLM probability of the fifth sentence given the first four sentences.  
We use both of these probabilities as classification features.

In addition, we also apply a third feature:
\begin{equation}
\frac{p_\theta(\textrm{ending} \mid
  \textrm{story})}{p_\theta(\textrm{ending})} \label{eq:ratio}
\end{equation}

The intuition is that a \emph{correct} ending should be unsurprising (to the model) given the four preceding sentences of the story (the numerator), controlling for the inherent surprise of the words in that ending (the denominator). 
%We use these three probabilities as classification features.

\paragraph{Stylistic Features.}
We hypothesize that {\it right} and {\it wrong} endings might be distinguishable using style features.
We adopt features that have been shown useful in the past in tasks such as detection of age \cite{Schler:2006,Rosenthal:2011,nguyen:2011:latech}, gender  \cite{Argamon:2003,Schler:2006,bamman2014gender}, and native language
\cite{Koppel:2005,Tsur:2007,Bergsma:2012}.

We add the following features to capture style differences between the two endings. 
These features are computed on the story endings alone, and do not consider the first four (shared) sentences of each story.
\begin{itemize}
\item\textit{\textbf{Length}.} The number of words in the sentence.
\item\textit{\textbf{Word n-grams.}} We use sequences of 1-5 words. Following \cite{Tsur:2010,Schwartz:2013}, we distinguish between high frequency and low frequency words. 
Specifically, we replace content words, which are often low frequency, with their part-of-speech tags (Nouns, Verbs, Adjectives and Adverbs).
\item\textit{\textbf{Character n-grams.}} Character n-grams are useful features in the detection of author style \cite{Stamatatos:2009} or language identification \cite{lui2011cross}.
We use character 4-grams.
\end{itemize}



\subsection{Experimental Setup}
We use  Python's sklearn logistic regression implementation with $L_2$
regularization, performing grid search on the development set to
tune a single hyperparameter -- the regularization parameter.  

We train the RNNLM using a single-layer LSTM of hidden dimension 512.
We use the ROC Stories for training, setting aside 10\% for validation of the language model.\footnote{We train on both the Spring 2016 and the Winter 2017 datasets, a total of roughly 100K stories}
We replace all words occurring less than 3 times by a special out-of-vocabulary character, yielding a vocabulary size of 21,582.
Only during training, we apply a dropout rate of 60\% while running the LSTM over all 5 sentences of the stories. 
Using AdamOptimizer \cite{kingma2014adam} and a learning rate of
$\eta=.001$, we train to minimize cross-entropy.

For computing the style features, we keep n-gram (character or word) features that occur at least five times in the training set.
All stylistic feature values are normalized to [0-1].
For the POS features, we tag all endings with the Spacy POS tagger.\footnote{\url{spacy.io/}}
The total number of features used by our system is 7,651.


\section{Results}

\begin{table}%[!t]
\begin{center}
%\small
\begin{tabular}{|l|r|} \hline
{\bf Model} & {\bf Accuracy} \\ \hline
{DSSM} \cite{Mostafazadeh:2016} & 0.585 \\ 
{LexVec} \cite{Salle:2016} & 0.599 \\ \hline\hline
%{Niko (shared task)}	& 0.700\\ 
%{tbmihaylov (shared task)} & 0.711\\ \hline\hline
{RNNLM Features}		& 0.677 \\ 
{Stylistic features} & {0.724} \\ 
{\bf Combined (Style + RNNLM)} & {\bf 0.752} \\ \hline\hline
Human judgment & 1.000 \\ \hline
\end{tabular}
\end{center}
\caption{\label{cloze_results}
Results on the test set of the  story cloze task. 
The first block are published results, the second block are our results.
LexVec results are taken from \cite{Speer:2016}.
Human judgement scores are taken from \cite{Mostafazadeh:2016}. 
}
\end{table}

% Show results
The performance of our system is described in Table \ref{cloze_results}. 
With $75.2\%$ accuracy, our system achieves $15.3\%$ better than the published state of the art \cite{Salle:2016}. 
%\ms{At the time of submission, we beat everyone or we're ranked XX in the leaderboard.}
The table also shows an analysis of the different features types used by our system.
While our RNNLM features alone reach 67.7\%, the style features perform better -- 72.4\%. 
This suggests that while this task is about story understanding, 
there is some information contained in stylistic features, which are slightly less sensitive to content.
As expected, the RNNLM features complement the stylistic ones, boosting performance by 7.5\% (over the RNNLM) and 2.8\% (over the style features). 
%This suggests that while the stylistic features provide useful signal, content-oriented models also contribute information. 
%\ms{Perhaps, an even more complex story understanding model would provide an even greater boost.}

\com{
\subsection{Further analysis} 

\begin{table}[h]
\begin{center}
%\small
\begin{tabular}{|c|c|} \hline
\textit{\textbf{Right}} & \textit{\textbf{Wrong}}\\ \hline
`ally' & ` hate'\\ \hline
`VBD the' & ` hat'\\ \hline
`START RB' & `START NNP'\\ \hline
`ved ' & `ated'\\ \hline
` tim' & `NN .'\\ \hline

\end{tabular}
\end{center}
\caption{\label{exp1_features}}
The top 5 most discriminative features for predicting {\it right} vs.~{\it wrong} endings.\end{table}

\ms{Don't know if this steps onto our other paper's toes.}
We analyze which features are most predictive of a \textit{wrong} or \textit{right} ending.
Table \ref{exp1_features} shows the highest weighted features using the logistic regression coefficients. 
The table shows a few interesting trends. 
First, authors tend to structure their sentences differently when writing {coherent}  vs. {incoherent} endings.
For instance, {coherent} endings are more likely to start with an adverb (e.g., ``then'', ``so'', ``eventually''), while {incoherent} ones tend to start with a proper noun.
In addition, we find that {incoherent} endings are more likely to
finish the sentence with a common noun.  %\nascomment{note to self to come backto this after we agree on methodology.}

More interestingly, the different writing tasks seem to impose a specific sentiment on the writer. 
Three of the top four most salient features for detecting {\it wrong} endings are variants of the verb ``hate''.
This indicates that when authors are asked to write {\it wrong} text, they tend to use negative language.

An alternative explanation to this hypothesis might be that the first four sentences of the stories in the ROC story corpus tend to be positive, and thus in order to make an ending {\it wrong}, authors adopted a negative approach. 
A similar idea was suggested in the original story cloze paper, where two sentiment-based baselines were evaluated. 
These baselines measured the relative sentiment between the ending and the previous sentences.
The performance of both these baselines was roughly chance-level, which seems to suggest that this is not entirely the case.
}

% ablation study? Table 2 from the paper sort of?

\section{Conclusion}
This paper described our submission to the LSDSem 2017 Shared Task. 
Our system leveraged both neural language model features and  stylistic features, achieving $75.2\%$ accuracy on the classification task. 
% We achieve $75.2\%$ which is ranked XXth in the ongoing codalab competition?
% Say something like:
\com{While we achieved state of the art performance, we still only achieved 75\% accuracy. 
This means the task is far from being solved. 
We need better tools to understand the common sense knowledge embedded in these stories.}

\bibliography{acl2017}
\bibliographystyle{eacl2017}


\end{document}