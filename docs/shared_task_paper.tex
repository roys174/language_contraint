\documentclass[11pt]{article}
\usepackage{eacl2017}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym,xcolor}

% \eaclfinalcopy % Uncomment this line for the final submission
%\def\eaclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}
% Editing macros
\newcommand{\my}[1]{\footnote{\color{red}{\textbf{#1}}}}

\newcommand{\ms}[1]{{\color{cyan}\{\textit{#1}\}$_{ms}$}}
\newcommand{\roy}[1]{\footnote{\color{red}{\textbf{Roy: #1}}}}
\newcommand{\royb}[2]{{\color{red}{\sout{#1}}}{\color{green}{#2}}}
\newcommand{\royc}[3]{\royb{#1}{#2}\roy{#3}}
\newcommand{\yc}[1]{{\color{bblue}\{\textit{#1}\}$_{yc}$}}

\title{Story Cloze task -- UW System}

\author{Roy Schwartz\\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And	
  Maarten Sap \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract} % This is my real quick try at this lol
As an effort to further commonsense short story understanding, Mostafazadeh, et al. ~(2016)\nocite{Mostafazadeh:2016} developed the \textit{Story Cloze Task}, now the 2017 LSD Sem shared task. This paper describes the University of Washington's submitted system.
We use stylistic and text comprehension features to determine which of two endings best fits a short story.
We achieve $75.1\%$ accuracy.
\end{abstract}

\section{Introduction}
% Describe the overall landscape of the problem
%  - Commonsense knowledge
%  - Script Learning
%  - 

Learning of commonsense knowledge is one of AI's biggest challenges, since it is not usually found in knowledge bases.
Coming up with the right types of data to analyze has been hard, and models are also not working well.

One particular type of commonsense knowledge that has gotten attention is story understanding \cite{??}. Stories have narratives that require understanding of how events typically flow and of what events are coherent after others.
Related to understanding event sequences is script learning, which focuses on stereotypical chains of events.

% Mention the specific purpose of the shared task (i.e. from the website)
The 2017 LSD Sem Shared Task provide a testbed to further commonsense story understanding: the \textit{Story Cloze} task \cite{Mostafazadeh:2016}. Specifically designed for the purpose of facilitating the learning of commonsense knowledge, the task consist of finding the \textit{corrent} ending to four sentence short stories, out of two possible endings. 
With the two-ending stories, Mostafazadeh, et al. ~(2016)\nocite{Mostafazadeh:2016} also released nearly $100$k five-sentence short stories (ROC Stories), to facilitate learning of commonsense narratives.

% Last paragraph should be a quick description of our system.
In this paper, we describe the system submission from the University of Washington NLP group (UW NLP).
We implement a simple classifier that uses surface features as well as language model features. Using the Shared Task test set, we achieve $75.1\%$ accuracy.

\section{Task summary}
% this section helps clarify some of the nomenclature?
\ms{note: I don't know if we're emphasizing ``story understanding'' too much, given that we're really not doing that?}
The \textit{Story Cloze Task} \cite{Mostafazadeh:2016}, selected as the 2017 LSD Sem Shared Task, aims to test commonsense story understanding. Towards that goal, two waves of short story collection were done.
\ms{Below is mostly copy-pasted from the paper, with a little of editing to remove our langConstraint angle.}
\paragraph{ROC Stories.}
The ROC Story Corpus consists of 49,255 five-sentence commonsense stories, collected on Amazon Mechanical Turk (AMT).\footnote{Recently, an additional 53K stories were released, which results in roughly 100K stories.}
Workers were instructed to write a coherent self-contained story, which has a clear beginning and end. 
To collect a broad spectrum of commonsense knowledge, there was no imposed subject for the stories,
which resulted in a wide range of different topics.
\paragraph{Story Cloze Task.}
After compiling the story corpus, the {\it Story Cloze Task} -- a task based on the corpus -- was introduced.
A subset of the stories was selected, and only the first four sentences of each story were presented to AMT workers.
Workers were asked to write a pair of new story endings for each story context: one {\it right} and one {\it wrong}.
Both endings are required to complete the story using one of the characters in the story context. 
Additionally,  the ending is required to be ``realistic and sensible'' \cite{Mostafazadeh:2016} when read out of context.

The resulting stories, both {\it right} and {\it wrong}, were then individually rated for coherence and meaningfulness. 
Only stories rated as simultaneously coherent with a {\it right} ending and neutral with a {\it wrong} ending were selected for the task. 

Based on these new stories, the task becomes: given a pair of stories that differ only in their endings, how well can the system determine which ending is {\it right} and which is {\it wrong}?

Mostafazadeh, et al. ~(2016)\nocite{Mostafazadeh:2016} split the Story Cloze stories into a development and a test set, each containing $1,871$ stories.

\subsection{Data}
While the ROC Stories provide data to do unsupervised learning of how stories should end, our goal is to be able to determine whether an ending is the \textit{right} one or the \textit{wrong} one. To that extent, we use the development set to train a classifier using features of the individual endings. We split that development set (90/10) into training and development.
The final size of our training/development/test sizes are 3,366/374/3,742 endings, respectively. 

It is worth nothing that our classification task is a slightly different take on the {\it Story cloze task}. 
Instead of classifying pairs of endings, one which is {\it right} and another which is {\it wrong}, we take the set of  {\it right} endings as positive samples and the set of {\it wrong} endings as our negative examples. 


\section{System Description}
%% Explain that we have two sets of features, style and language.
%% Maybe mention here that we're doing training on validation set ?
In our system, we implement a simple linear classifier to determine whether and ending is the \textit{correct} one or the \textit{incorrect} one.
Specifically, we use the logistic regression implementation by Scikit-Learn \cite{sklearn}, regularizing using an L2 loss.
We grid search the regularization parameter on our development set. 
\ms{We should maybe add one of two more sentences?}

\subsection{Features}
We use two types of features (described below), designed to capture different dimensions of the problem. We use \textit{stylistic} features to capture differences in writing between ``coherent'' endings and ``incoherent'' ones. We opt for \textit{language model} features to leverage corpus level word distributions, specifically longer term sequence probabilities.
Our end system makes use of $XX$ distinct features. \ms{Would be nice to add that number?}
\paragraph{Stylistic Features}
%% take stuff from the paper
In our classification task, we add the following features to capture style differences between the two endings:
\begin{itemize}
\item\textit{\textbf{Length}.} The number of words in the sentence.
\item\textit{\textbf{Word n-grams.}} We use sequences of 1-5 words. Following \cite{Tsur:2010,Schwartz:2013}, we distinguish between high frequency and low frequency words. 
Specifically, we replace content words, which are often low frequency, with their part-of-speech tags (Nouns, Verbs, Adjectives and Adverbs).
\item\textit{\textbf{Character n-grams.}} Character n-grams are useful features in author style \cite{Stamatatos:2009} or language identification \cite{lui2011cross}.
We use character 4-grams.
\end{itemize}
For computing our features, we keep n-gram (character or word) features that occur at least five times in the training set.
All stylistic feature values are normalized to [0-1].
For the POS features, we tag all endings with the Spacy POS tagger.\footnote{\url{spacy.io/}}

\paragraph{Language Model Features}
%% take the RLM paragraph from the paper
In addition to stylistic features, we experiment with state-of-the-art text comprehension models, specifically a recurrent language model (RLM, Mikolov, et al. (2010)\nocite{mikolov2010recurrent}).
We train the RLM using a single-layer LSTM \cite{hochreiter1997long} of hidden dimension $h=512$.
We use the ROC Stories for training, setting aside $10\%$ for validation of the language model. 
We replace all words occurring less than 3 times by a special out-of-vocabulary character, yielding a vocabulary size of $|V|=21,582$.
Only during training, we apply a dropout rate of 60\% while running the LSTM over all 5 sentences of the stories. 
Using AdamOptimizer \cite{kingma2014adam} and a learning rate of $\eta=.001$, we train with backpropagation on cross-entropy. % minibatches of $50$ stories.
\section{Results \& Discussion}
% Show results

% ablation study? Table 2 from the paper sort of?

\section{Conclusion}

% Our system makes use of knowledge brought to us by the constrained writing literature (?)
% talk about how we use state-of-the-art language modelling tools.
% We achieve $75.1\%$ which is ranked XXth in the ongoing codalab competition.

\bibliography{acl2017}
\bibliographystyle{eacl2017}


\end{document}